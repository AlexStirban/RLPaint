{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid warning from TF\n",
    "\n",
    "# Full imports\n",
    "import gym\n",
    "import mlflow\n",
    "\n",
    "# Aliased imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Partial Import\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember to export to export \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\"\" if using linux\n",
    "# Drop numa errors in term: \"for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\"\n",
    "\n",
    "# Check if we have GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## META CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_ENV = \"CartPole-v1\"\n",
    "EXPERIMENT_NAME = \"cart_pole_a2c\"\n",
    "TAGS = {\n",
    "    \"type\": \"RL\",\n",
    "    \"env\": \"Discrete Cart Pole\",\n",
    "    \"algorithm\": \"A2C\",\n",
    "    \"sub-algorithm\": \"Hybrid Model\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL AUX DEFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): _description_\n",
    "        \"\"\"\n",
    "\n",
    "        # Call super to properly init\n",
    "        super().__init__()\n",
    "\n",
    "        # Define model\n",
    "        self.base_1 = tf.keras.layers.Dense(128)\n",
    "        self.activation_1 = tf.keras.layers.LeakyReLU()\n",
    "        self.actor_out = tf.keras.layers.Dense(env.action_space.n, activation=\"softmax\")\n",
    "        self.critic_out = tf.keras.layers.Dense(1)\n",
    "\n",
    "        \n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: _description_\n",
    "        \"\"\"\n",
    "        # Apply base layers\n",
    "        x = self.base_1(inputs)\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        # Compute distribution of actions for actor and pick one and corresponding log_prob\n",
    "        #action_dist = tfp.distributions.Categorical(probs=self.actor_out(x), dtype=tf.int32)\n",
    "        #action = action_dist.sample()\n",
    "\n",
    "        # Compute estimate for value\n",
    "        #value = self.critic_out(x)\n",
    "\n",
    "        return self.actor_out(x), self.critic_out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLUtils:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def r_to_g(rewards: tf.Tensor, gamma: float, std: bool = True) -> tf.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            rewards (tf.Tensor): _description_\n",
    "            gamma (float): _description_\n",
    "            std (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: _description_\n",
    "        \"\"\"\n",
    "        # Prepare aux vars\n",
    "        rewards = tf.cast(rewards, dtype=tf.float32)\n",
    "        t = tf.range(tf.size(rewards), dtype=tf.float32)\n",
    "\n",
    "        # Compute factors\n",
    "        delta = rewards * gamma ** t\n",
    "        g = tf.cumsum(delta[::-1])[::-1] / gamma ** t\n",
    "\n",
    "        # Std if needed\n",
    "        if std:\n",
    "            g = (g - tf.reduce_mean(g)) / (tf.math.reduce_std(g) + EPS)\n",
    "        \n",
    "        # Return expected returns\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env, gamma: float, model: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer) -> None:\n",
    "        \"\"\"Returns an agent acting on \"env\" using the specified \"gamma\" and \"model\".\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): Gym Envinronment\n",
    "            gamma (float): Discount factor\n",
    "            model (tf.keras.Model): Keras model\n",
    "        \"\"\"\n",
    "\n",
    "        # Init private props\n",
    "        self._env = env\n",
    "        self._gamma = gamma\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "        # Init public props\n",
    "        #\n",
    "\n",
    "    def _aux_np_step(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Intermediate function used to perform steps using tensors.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): Action to be perfomed\n",
    "\n",
    "        Returns:\n",
    "            state (np.ndarray): new state\n",
    "            reward (np.ndarray): reward obtained\n",
    "            done (np.ndarray): indicates if episode finished\n",
    "        \"\"\"\n",
    "\n",
    "        # Perform step\n",
    "        state, reward, done, _, _ = self._env.step(action)\n",
    "\n",
    "        # Cast to 0/1 to exploit tensor repr\n",
    "        return (\n",
    "            state.astype(np.float32),\n",
    "            np.array(reward, np.float32),\n",
    "            np.array(done, np.int32)\n",
    "        )\n",
    "\n",
    "    def _aux_tf_step(self, action: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Auxiliar function used to perform steps using tensors.\n",
    "\n",
    "        Args:\n",
    "            action (tf.Tensor): action to be performed\n",
    "\n",
    "        Returns:\n",
    "            state (tf.float32): new state\n",
    "            reward (tf.int32): reward obtained\n",
    "            done (tf.int32): indicates if episode finished\n",
    "        \"\"\"\n",
    "        return tf.numpy_function(self._aux_np_step, [action], [tf.float32, tf.float32, tf.int32])\n",
    "\n",
    "    def _exec_trajectory(self, init_state: tf.Tensor, model: tf.keras.Model, max_steps: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            init_state (tf.Tensor): _description_\n",
    "            max_steps (tf.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define buffers\n",
    "        log_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        rewards =  tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "        # Set current state\n",
    "        state = init_state\n",
    "\n",
    "        for i in tf.range(max_steps):\n",
    "            # state_i (n,) -> state_f (1, n)\n",
    "            state = tf.expand_dims(state, axis=0)\n",
    "\n",
    "            # Pick an action and get corresponding log prob and value\n",
    "            action_probs, value = model(state)\n",
    "            action_dist = tfp.distributions.Categorical(probs=action_probs, dtype=tf.int32)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "\n",
    "            # Perform next action\n",
    "            state, reward, done = self._aux_tf_step(action[0])\n",
    "            state.set_shape(init_state.shape)\n",
    "\n",
    "            # Save in buffer\n",
    "            log_probs.write(i, tf.squeeze(log_prob))\n",
    "            values.write(i, tf.squeeze(value))\n",
    "            rewards.write(i, reward)\n",
    "\n",
    "            # Break if we finished the run\n",
    "            if tf.cast(done, tf.bool):\n",
    "                break\n",
    "        \n",
    "        # Stack into a single tensor\n",
    "        log_probs = log_probs.stack()\n",
    "        values = values.stack()\n",
    "        rewards = rewards.stack()\n",
    "\n",
    "        return log_probs, values, rewards\n",
    "    \n",
    "\n",
    "    def _loss(self, log_probs: tf.Tensor, values: tf.Tensor, returns: tf.Tensor) -> tf.Tensor:\n",
    "        adv = returns - values\n",
    "\n",
    "        # Actor loss\n",
    "        actor_loss = -tf.math.reduce_sum(log_probs * adv)\n",
    "\n",
    "        # Critic loss\n",
    "        # Use hubber loss because it's more stable to outliers than delta^2\n",
    "        critic_loss = huber_loss(values, returns)\n",
    "\n",
    "        return actor_loss + critic_loss\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _train(self, init_state: tf.Tensor, model: tf.keras.Model, max_steps: int) -> tf.Tensor:\n",
    "        \"\"\"_summary_p\n",
    "\n",
    "        Args:\n",
    "            init_state (tf.Tensor): _description_\n",
    "            max_steps (int): _description_\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: _description_\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:    \n",
    "            tape.watch(init_state)\n",
    "            tape.watch(model.trainable_variables)\n",
    "                \n",
    "            log_probs, values, rewards = self._exec_trajectory(init_state, model, max_steps)\n",
    "            returns = RLUtils.r_to_g(rewards, self._gamma)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self._loss(log_probs, values, returns)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # Update weights\n",
    "        self._optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Retrun rewards\n",
    "        return tf.math.reduce_sum(rewards)\n",
    "\n",
    "    \n",
    "    def train(self, init_state: np.ndarray, max_steps: int):\n",
    "        init_state = tf.constant(init_state, dtype=tf.float32)\n",
    "        return int(self._train(init_state, self._model, max_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b9cc05d2fe4194b5e8d82cf985d8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_17521/1965587532.py\", line 143, in _train  *\n        self._optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"/home/main/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 689, in apply_gradients  **\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/home/main/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['dense_63/kernel:0', 'dense_63/bias:0', 'dense_64/kernel:0', 'dense_64/bias:0', 'dense_65/kernel:0', 'dense_65/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_63/kernel:0' shape=(4, 128) dtype=float32>), (None, <tf.Variable 'dense_63/bias:0' shape=(128,) dtype=float32>), (None, <tf.Variable 'dense_64/kernel:0' shape=(128, 2) dtype=float32>), (None, <tf.Variable 'dense_64/bias:0' shape=(2,) dtype=float32>), (None, <tf.Variable 'dense_65/kernel:0' shape=(128, 1) dtype=float32>), (None, <tf.Variable 'dense_65/bias:0' shape=(1,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/main/Documents/Projects/RLPaint/Playground/Cart Pole/cart_pole_a2c.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m (tbar \u001b[39m:=\u001b[39m trange(N_EPISODES)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     init_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     reward_e \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(init_state, MAX_STEPS)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Save rewards\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward_e)\n",
      "\u001b[1;32m/home/main/Documents/Projects/RLPaint/Playground/Cart Pole/cart_pole_a2c.ipynb Cell 12\u001b[0m in \u001b[0;36mAgent.train\u001b[0;34m(self, init_state, max_steps)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, init_state: np\u001b[39m.\u001b[39mndarray, max_steps: \u001b[39mint\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     init_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(init_state, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/main/Documents/Projects/RLPaint/Playground/Cart%20Pole/cart_pole_a2c.ipynb#X14sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(init_state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model, max_steps))\n",
      "File \u001b[0;32m~/anaconda3/envs/rlenv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5j4h4y67.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___train\u001b[0;34m(self, init_state, model, max_steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_loss, (ag__\u001b[39m.\u001b[39mld(log_probs), ag__\u001b[39m.\u001b[39mld(values), ag__\u001b[39m.\u001b[39mld(returns)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     17\u001b[0m grads \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(loss), ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 18\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mapply_gradients, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(grads), ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39m, grads_and_vars, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    650\u001b[0m ):\n\u001b[1;32m    651\u001b[0m     \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[39m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[1;32m    690\u001b[0m     var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    692\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[1;32m    693\u001b[0m         \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         (\n\u001b[1;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     89\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_17521/1965587532.py\", line 143, in _train  *\n        self._optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"/home/main/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 689, in apply_gradients  **\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/home/main/anaconda3/envs/rlenv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['dense_63/kernel:0', 'dense_63/bias:0', 'dense_64/kernel:0', 'dense_64/bias:0', 'dense_65/kernel:0', 'dense_65/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_63/kernel:0' shape=(4, 128) dtype=float32>), (None, <tf.Variable 'dense_63/bias:0' shape=(128,) dtype=float32>), (None, <tf.Variable 'dense_64/kernel:0' shape=(128, 2) dtype=float32>), (None, <tf.Variable 'dense_64/bias:0' shape=(2,) dtype=float32>), (None, <tf.Variable 'dense_65/kernel:0' shape=(128, 1) dtype=float32>), (None, <tf.Variable 'dense_65/bias:0' shape=(1,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "tapes gradient with class methodsLR = 1e-3\n",
    "GAMMA = 0.99\n",
    "N_EPISODES = 10000\n",
    "MAX_STEPS = 500\n",
    "\n",
    "# Define basic vars\n",
    "env = gym.make(DEFAULT_ENV)\n",
    "model = ActorCritic(env)\n",
    "model.build(input_shape=(None, env.observation_space.shape[0]))\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LR)\n",
    "agent = Agent(env, GAMMA, model, optimizer=optimizer)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    for e in (tbar := trange(N_EPISODES)):\n",
    "        init_state = env.reset()[0]\n",
    "        reward_e = agent.train(init_state, MAX_STEPS)\n",
    "\n",
    "        # Save rewards\n",
    "        rewards.append(reward_e)\n",
    "\n",
    "        # Update progressbar\n",
    "        tbar.set_postfix(reward=reward_e)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bc5bdd1f96b0bfe1b0a7578d394c815a18b7bbf3d83f21c57372441a2e86538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
