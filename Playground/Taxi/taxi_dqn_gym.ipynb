{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving TAXI Environment using QTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full imports\n",
    "import gym\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Aliased imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Partial Import\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m1000\u001b[39m): \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RLEnv/lib/python3.9/site-packages/tqdm/notebook.py:324\u001b[0m, in \u001b[0;36mtnrange\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtnrange\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    320\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39m    A shortcut for `tqdm.notebook.tqdm(xrange(*args), **kwargs)`.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m    On Python3+, `range` is used instead of `xrange`.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mreturn\u001b[39;00m tqdm_notebook(_range(\u001b[39m*\u001b[39;49margs), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RLEnv/lib/python3.9/site-packages/tqdm/notebook.py:243\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    242\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RLEnv/lib/python3.9/site-packages/tqdm/notebook.py:118\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    120\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "for _ in trange(1000): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to export to export \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\"\" if using linux\n",
    "# Drop numa errors in term: \"for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\"\n",
    "\n",
    "\n",
    "# We have GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make videos folder\n",
    "# Install asciinema: \"apt-get install asciinema\"\n",
    "%mkdir videos\n",
    "%rm -rf ./logs\n",
    "%mkdir logs\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the a grid of 5x5, where the pick-up and drop-off locations might be {R, G, B, Y}. Since the client might be a passanger, we need to account with an additional state:\n",
    "* States of the taxi position: $5\\times5$\n",
    "* States of the drop-off locations: $4$\n",
    "* States of the pick-up/passenger: $4$ (R, G, B, Y) + $1$ (on taxi) \n",
    "\n",
    "Then we have $5\\times5\\times5\\times(4+1)=500$ total states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a basic DQN model using experience replay and target network:\n",
    "* **Experience Replay**: Updating the NN in an online manner using the sequential states introduces correlation in the training process, making it unstable. To reduce  this problem we'll save past experiencias and update the network in random minibatches, allowing us to revisit rare occurences and learn more from individual experiences.\n",
    "\n",
    "* **Target Network**: Directly updating Q'(s, a) using as the \"real\" value (Q(s, a)) the one provided by the Bellman equation is risky. Even thought there's just one step of difference using the Bellman equation and thus making the estimation relatively accurate, distinguishing between Q(s, a) and the Bellman estimation proves difficult for a NN. Finally, altering the value of Q(s, a) might indirectly affect the value of Q(s', a'). To address the issue we'll use a copy of the main NN that's we'll use to compute Q(s', a') and update periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV = \"Taxi-v3\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 999985\n",
    "EPS_MIN = 0.02\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "SYNC_TARGET_FRAMES = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(DEFAULT_ENV)\n",
    "env.reset()\n",
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic model using the embedding layer\n",
    "# We have discrete values\n",
    "dqn = tf.keras.Sequential([\n",
    "    # Input\n",
    "    tf.keras.layers.Embedding(env.observation_space.n, env.action_space.n, input_length=1),\n",
    "\n",
    "    tf.keras.layers.Dense(32),\n",
    "\n",
    "    # Output\n",
    "    tf.keras.layers.Dense(6),\n",
    "    tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "print(dqn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience\n",
    "Experience = collections.namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"new_state\", \"done\"])\n",
    "\n",
    "# Define deque using experince structure\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "        self.max_buffer_size = buffer_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, state, action, reward, new_state, done):\n",
    "        self.buffer.append(Experience(state, action, reward, new_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Get indexes\n",
    "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        # Get values\n",
    "        return list(zip(*[self.buffer[i] for i in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, model, lr=0.001, gamma=0.7, eps_i=1, eps_f=0.02, eps_d=0.9998, batch_size=32, buffer_size=10000, sync_target_frames=1000):\n",
    "        # Save env\n",
    "        self.env = env\n",
    "\n",
    "        # Save model\n",
    "        self.model = model\n",
    "\n",
    "        # Compile it using the adam\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        #self.model.compile(optimizer=self.optimizer , loss=\"mse\")\n",
    "\n",
    "        # Define target model\n",
    "        self.target_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "        # Init experience buffer\n",
    "        self.buffer = ExperienceReplayBuffer(buffer_size)\n",
    "\n",
    "        # Def loss\n",
    "        self.loss_fn = mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Init sate\n",
    "        self._reset()\n",
    "\n",
    "        # Save hyperparams\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps = self.eps_i = eps_i\n",
    "        self.eps_f = eps_f\n",
    "        self.eps_d = eps_d\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_target_frames = sync_target_frames\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state, _ = self.env.reset()\n",
    "\n",
    "    def step(self):\n",
    "        # Decay epsilon\n",
    "        # This will allow us to transition from exploration to explotaton as the model\n",
    "        # model performs better\n",
    "        self.eps = max(self.eps * self.eps_d, self.eps_f)\n",
    "\n",
    "        # Perform action using e-greedy\n",
    "        if np.random.random() < self.eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            available_actions = self.model(np.array(self.state).reshape(-1, 1))\n",
    "            action = np.argmax(available_actions)\n",
    "\n",
    "        # Get new action\n",
    "        data = self.env.step(action)\n",
    "        state_new, reward, done, _, _ = data\n",
    "\n",
    "        # Save experience \n",
    "        self.buffer.append(self.state, action, reward, state_new, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = state_new\n",
    "\n",
    "        # If we have reached our goal\n",
    "        # reset the goal\n",
    "        if done:\n",
    "            self._reset()\n",
    "        \n",
    "        return reward, done\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, curr_states, actions, next_states, rewards, done_mask):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get Q Values for performed action\n",
    "            curr_q_values_all = self.model(curr_states)\n",
    "            curr_q_values_sel = tf.gather(curr_q_values_all, actions, batch_dims=1)\n",
    "\n",
    "            # Get max Q from target network\n",
    "            next_q_values_all = tf.stop_gradient(self.target_model(next_states))\n",
    "            next_q_values_max = tf.reduce_max(next_q_values_all, axis=1)\n",
    "\n",
    "            # Mask done actions\n",
    "            next_q_values_max = tf.where(done_mask, tf.constant(0, dtype=float), next_q_values_max)\n",
    "            \n",
    "            # Compute expected Q values\n",
    "            \n",
    "            expected_q = tf.constant(self.gamma) * next_q_values_max + rewards\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.loss_fn(curr_q_values_sel, expected_q)\n",
    "            \n",
    "            # Update weights\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            \n",
    "        \n",
    "    def train(self, sync=False):\n",
    "        # If we don't have enough we don't train\n",
    "        if len(self.buffer) < self.buffer.max_buffer_size:\n",
    "            return\n",
    "\n",
    "        # Get sample from batch\n",
    "        sample = np.array(self.buffer.sample(self.batch_size)).T\n",
    "\n",
    "        # As tensors\n",
    "        current_states = tf.constant(sample[:, 0])\n",
    "        next_states = tf.constant(sample[:, 3])\n",
    "        actions = tf.constant(sample[:, 1])\n",
    "        rewards = tf.constant(sample[:, 2], dtype=float)\n",
    "        done_mask = tf.constant(sample[:, 4] == 1)\n",
    "\n",
    "        # Train\n",
    "        self._train_step(current_states, actions, next_states, rewards, done_mask)\n",
    "\n",
    "        # Sync with target\n",
    "        if sync:\n",
    "            self.target_model.set_weights(self.model.get_weights())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEFAULT_ENV = \"Taxi-v3\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LEARNING_RATE = 1e-3 \n",
    "GAMMA = 0.7\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999965\n",
    "EPS_MIN = 0.02\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "SYNC_TARGET_FRAMES = 200\n",
    "\n",
    "\n",
    "# Agent\n",
    "agent = Agent(env, dqn, LEARNING_RATE, GAMMA, EPS_START, EPS_MIN, EPS_DECAY, BATCH_SIZE, REPLAY_BUFFER_SIZE)\n",
    "n_frames = 0\n",
    "stats = {}\n",
    "\n",
    "for e in (tbar := trange(10000)):\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    while not done:\n",
    "        # Take step\n",
    "        n_frames += 1\n",
    "        r, done = agent.step()\n",
    "\n",
    "        # Add rewards\n",
    "        rewards.append(r)\n",
    "\n",
    "        # Train\n",
    "        agent.train(sync=(n_frames % SYNC_TARGET_FRAMES == 0))\n",
    "\n",
    "    # Update bar\n",
    "    tbar.set_description(f\"Mean reward: {np.array(rewards).mean(): 0.3}, Epsilon: {agent.eps: 0.3}\")\n",
    "    tbar.refresh()\n",
    "\n",
    "    # Update stats\n",
    "    stats[e] = np.array(rewards).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pd = pd.DataFrame.from_dict(stats, orient=\"index\")\n",
    "stats_pd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16fe4abf8e200b9e84ebcaf7ad1cf3f663399e470120c5ef77a6f18e61022f11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
