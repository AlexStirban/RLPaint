{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving TAXI Environment using QTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full imports\n",
    "import gym\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Aliased imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Partial Import\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to export to export \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\"\" if using linux\n",
    "# Drop numa errors in term: \"for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\"\n",
    "\n",
    "\n",
    "# We have GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make videos folder\n",
    "# Install asciinema: \"apt-get install asciinema\"\n",
    "%mkdir videos\n",
    "%rm -rf ./logs\n",
    "%mkdir logs\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the a grid of 5x5, where the pick-up and drop-off locations might be {R, G, B, Y}. Since the client might be a passanger, we need to account with an additional state:\n",
    "* States of the taxi position: $5\\times5$\n",
    "* States of the drop-off locations: $4$\n",
    "* States of the pick-up/passenger: $4$ (R, G, B, Y) + $1$ (on taxi) \n",
    "\n",
    "Then we have $5\\times5\\times5\\times(4+1)=500$ total states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a basic DQN model using experience replay and target network:\n",
    "* **Experience Replay**: Updating the NN in an online manner using the sequential states introduces correlation in the training process, making it unstable. To reduce  this problem we'll save past experiencias and update the network in random minibatches, allowing us to revisit rare occurences and learn more from individual experiences.\n",
    "\n",
    "* **Target Network**: Directly updating Q'(s, a) using as the \"real\" value (Q(s, a)) the one provided by the Bellman equation is risky. Even thought there's just one step of difference using the Bellman equation and thus making the estimation relatively accurate, distinguishing between Q(s, a) and the Bellman estimation proves difficult for a NN. Finally, altering the value of Q(s, a) might indirectly affect the value of Q(s', a'). To address the issue we'll use a copy of the main NN that's we'll use to compute Q(s', a') and update periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV = \"Taxi-v3\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 999985\n",
    "EPS_MIN = 0.02\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "SYNC_TARGET_FRAMES = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(DEFAULT_ENV)\n",
    "env.reset()\n",
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic model using the embedding layer\n",
    "# We have discrete values\n",
    "dqn = tf.keras.Sequential([\n",
    "    # Input\n",
    "    tf.keras.layers.Embedding(env.observation_space.n, env.action_space.n, input_length=1),\n",
    "\n",
    "    tf.keras.layers.Dense(32),\n",
    "\n",
    "    # Output\n",
    "    tf.keras.layers.Dense(6),\n",
    "    tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "print(dqn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience\n",
    "Experience = collections.namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"new_state\", \"done\"])\n",
    "\n",
    "# Define deque using experince structure\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "        self.max_buffer_size = buffer_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, state, action, reward, new_state, done):\n",
    "        self.buffer.append(Experience(state, action, reward, new_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Get indexes\n",
    "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        # Get values\n",
    "        return list(zip(*[self.buffer[i] for i in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, model, lr=0.001, gamma=0.7, eps_i=1, eps_f=0.02, eps_d=0.9998, batch_size=32, buffer_size=10000, sync_target_frames=1000):\n",
    "        # Save env\n",
    "        self.env = env\n",
    "\n",
    "        # Save model\n",
    "        self.model = model\n",
    "\n",
    "        # Compile it using the adam\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        #self.model.compile(optimizer=self.optimizer , loss=\"mse\")\n",
    "\n",
    "        # Define target model\n",
    "        self.target_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "        # Init experience buffer\n",
    "        self.buffer = ExperienceReplayBuffer(buffer_size)\n",
    "\n",
    "        # Def loss\n",
    "        self.loss_fn = mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Init sate\n",
    "        self._reset()\n",
    "\n",
    "        # Save hyperparams\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps = self.eps_i = eps_i\n",
    "        self.eps_f = eps_f\n",
    "        self.eps_d = eps_d\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_target_frames = sync_target_frames\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state, _ = self.env.reset()\n",
    "\n",
    "    def step(self):\n",
    "        # Decay epsilon\n",
    "        # This will allow us to transition from exploration to explotaton as the model\n",
    "        # model performs better\n",
    "        self.eps = max(self.eps * self.eps_d, self.eps_f)\n",
    "\n",
    "        # Perform action using e-greedy\n",
    "        if np.random.random() < self.eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            available_actions = self.model(np.array(self.state).reshape(-1, 1))\n",
    "            action = np.argmax(available_actions)\n",
    "\n",
    "        # Get new action\n",
    "        data = self.env.step(action)\n",
    "        state_new, reward, done, _, _ = data\n",
    "\n",
    "        # Save experience \n",
    "        self.buffer.append(self.state, action, reward, state_new, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = state_new\n",
    "\n",
    "        # If we have reached our goal\n",
    "        # reset the goal\n",
    "        if done:\n",
    "            self._reset()\n",
    "        \n",
    "        return reward, done\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, curr_states, actions, next_states, rewards, done_mask):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get Q Values for performed action\n",
    "            curr_q_values_all = self.model(curr_states)\n",
    "            curr_q_values_sel = tf.gather(curr_q_values_all, actions, batch_dims=1)\n",
    "\n",
    "            # Get max Q from target network\n",
    "            next_q_values_all = tf.stop_gradient(self.target_model(next_states))\n",
    "            next_q_values_max = tf.reduce_max(next_q_values_all, axis=1)\n",
    "\n",
    "            # Mask done actions\n",
    "            next_q_values_max = tf.where(done_mask, tf.constant(0, dtype=float), next_q_values_max)\n",
    "            \n",
    "            # Compute expected Q values\n",
    "            \n",
    "            expected_q = tf.constant(self.gamma) * next_q_values_max + rewards\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.loss_fn(curr_q_values_sel, expected_q)\n",
    "            \n",
    "            # Update weights\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            \n",
    "        \n",
    "    def train(self, sync=False):\n",
    "        # If we don't have enough we don't train\n",
    "        if len(self.buffer) < self.buffer.max_buffer_size:\n",
    "            return\n",
    "\n",
    "        # Get sample from batch\n",
    "        sample = np.array(self.buffer.sample(self.batch_size)).T\n",
    "\n",
    "        # As tensors\n",
    "        current_states = tf.constant(sample[:, 0])\n",
    "        next_states = tf.constant(sample[:, 3])\n",
    "        actions = tf.constant(sample[:, 1])\n",
    "        rewards = tf.constant(sample[:, 2], dtype=float)\n",
    "        done_mask = tf.constant(sample[:, 4] == 1)\n",
    "\n",
    "        # Train\n",
    "        self._train_step(current_states, actions, next_states, rewards, done_mask)\n",
    "\n",
    "        # Sync with target\n",
    "        if sync:\n",
    "            self.target_model.set_weights(self.model.get_weights())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEFAULT_ENV = \"Taxi-v3\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LEARNING_RATE = 1e-3 \n",
    "GAMMA = 0.7\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999965\n",
    "EPS_MIN = 0.02\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "SYNC_TARGET_FRAMES = 200\n",
    "\n",
    "\n",
    "# Agent\n",
    "agent = Agent(env, dqn, LEARNING_RATE, GAMMA, EPS_START, EPS_MIN, EPS_DECAY, BATCH_SIZE, REPLAY_BUFFER_SIZE)\n",
    "n_frames = 0\n",
    "stats = {}\n",
    "\n",
    "for e in (tbar := trange(10000)):\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    while not done:\n",
    "        # Take step\n",
    "        n_frames += 1\n",
    "        r, done = agent.step()\n",
    "\n",
    "        # Add rewards\n",
    "        rewards.append(r)\n",
    "\n",
    "        # Train\n",
    "        agent.train(sync=(n_frames % SYNC_TARGET_FRAMES == 0))\n",
    "\n",
    "    # Update bar\n",
    "    tbar.set_description(f\"Mean reward: {np.array(rewards).mean(): 0.3}, Epsilon: {agent.eps: 0.3}\")\n",
    "    tbar.refresh()\n",
    "\n",
    "    # Update stats\n",
    "    stats[e] = np.array(rewards).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pd = pd.DataFrame.from_dict(stats, orient=\"index\")\n",
    "stats_pd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
